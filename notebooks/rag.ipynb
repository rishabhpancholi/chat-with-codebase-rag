{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c657c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings,ChatGoogleGenerativeAI\n",
    "from langchain_community.document_loaders import GithubFileLoader\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter,Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de0c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "access_token = os.getenv(\"GITHUB_PERSONAL_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c064776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GithubFileLoader(\n",
    "    repo = \"rishabhpancholi/fake-job-detection-mlops-project\",\n",
    "    branch = \"main\",\n",
    "    access_token = access_token,\n",
    "    github_api_url = \"https://api.github.com\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\n",
    "        (\".py\",\".yml\",\".yaml\",\"Dockerfile\",\"md\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a1a2194",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0f2946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Github Actions Workflow\\nname: MLOPS CICD\\n\\n# When this workflow should run\\non:\\n  push:\\n    branches:\\n      - main # Runs the workflow whenever code is pushed to the main branch\\n\\njobs:\\n  mlops_workflow:\\n    runs-on: ubuntu-latest\\n    environment: production\\n\\n    env:\\n      MODEL_NAME: ${{ secrets.MODEL_NAME }}\\n      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\\n      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\\n      MLFLOW_TRACKING_USERNAME: ${{ secrets.MLFLOW_TRACKING_USERNAME }}\\n      MLFLOW_TRACKING_PASSWORD: ${{ secrets.MLFLOW_TRACKING_PASSWORD }}\\n      MODEL_TRAINER_REPO_OWNER: ${{ secrets.MODEL_TRAINER_REPO_OWNER }}\\n      MODEL_TRAINER_REPO_NAME: ${{ secrets.MODEL_TRAINER_REPO_NAME }}\\n      DVC_REMOTE_ACCESS_KEY_ID: ${{ secrets.DVC_REMOTE_ACCESS_KEY_ID }}\\n      DVC_REMOTE_SECRET_ACCESS_KEY: ${{ secrets.DVC_REMOTE_SECRET_ACCESS_KEY }}\\n\\n    steps: # Steps that run inside this job\\n      # Step 1: Checkout the repository to the runner\\n      - name: Checkout repository\\n        uses: actions/checkout@v4 # Official GitHub action to clone your repo\\n\\n      # Step 2: Set up a specific Python version for the environment\\n      - name: Set up Python\\n        uses: actions/setup-python@v5\\n        with:\\n          python-version: \"3.13\"\\n\\n      # Step 3: Install project dependencies\\n      - name: Install dependencies\\n        run: |\\n          python -m pip install --upgrade pip    \\n          pip install --no-cache-dir -r requirements.txt\\n          python -m spacy download en_core_web_sm\\n\\n      # Step 4: Run the ML pipeline\\n      - name: Run ML pipeline\\n        env:\\n          AWS_ACCESS_KEY_ID: ${{ secrets.DVC_REMOTE_ACCESS_KEY_ID }}\\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.DVC_REMOTE_SECRET_ACCESS_KEY }}\\n          AWS_DEFAULT_REGION: us-east-1\\n        run: |\\n          dvc pull\\n          dvc repro\\n          dvc push\\n\\n      # Step 5: Run all tests using pytest\\n      - name: Run tests\\n        run: |\\n          pytest -v\\n\\n      # Step 6: Login to AWS ECR\\n      - name: Login to Amazon ECR\\n        env:\\n          AWS_REGION: us-east-1\\n        run: |\\n          aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/j4r9x7x2\\n\\n      # Step 7: Build and push the Docker image\\n      - name: Build and push Docker image\\n        uses: docker/build-push-action@v5\\n        with:\\n          context: .\\n          push: true\\n          tags: public.ecr.aws/j4r9x7x2/rishabhpancholi/real-fake-job-detection-api:latest\\n\\n      # Step 8:\\n      - name: Deploy to EC2 instance\\n        uses: appleboy/ssh-action@v1.0.3\\n        env:\\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\\n          AWS_REGION: ap-south-1\\n        with:\\n          host: ${{ secrets.EC2_HOST }}\\n          username: ubuntu\\n          key: ${{ secrets.EC2_SSH_KEY }}\\n          script: |\\n            echo \"Logging to ECR\"\\n            aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/j4r9x7x2\\n            echo \"Stopping old container\"\\n            docker-compose down\\n            echo \"Removing old image\"\\n            docker image prune -af\\n            echo \"Running new container\"\\n            docker-compose up -d\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5617bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_type_tagger(documents):\n",
    "    for doc in documents:\n",
    "        if doc.metadata[\"path\"].endswith(\".yml\"):\n",
    "            doc.metadata[\"file_type\"] = \"yaml\"\n",
    "        else:\n",
    "            doc.metadata[\"file_type\"] = doc.metadata[\"path\"].split(\".\")[-1]\n",
    "        doc.metadata[\"file_name\"] = doc.metadata[\"path\"].split(\"/\")[-1].split(\".\")[0]\n",
    "    return documents\n",
    "\n",
    "documents = file_type_tagger(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc8c526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_code_parser(code):\n",
    "        tree = ast.parse(code)\n",
    "\n",
    "        functions = []\n",
    "        classes = []\n",
    "\n",
    "        for node in ast.walk(tree):\n",
    "           if isinstance(node, ast.FunctionDef):\n",
    "               name_node = node.name\n",
    "               functions.append(name_node)\n",
    "           elif isinstance(node, ast.ClassDef):\n",
    "               name_node = node.name\n",
    "               classes.append(name_node)\n",
    "\n",
    "        return functions,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a39c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    if doc.metadata[\"file_type\"] == \"py\":\n",
    "        functions,classes = python_code_parser(doc.page_content)\n",
    "        doc.metadata[\"functions\"] = functions\n",
    "        doc.metadata[\"classes\"] = classes\n",
    "    else:\n",
    "        doc.metadata[\"functions\"] = []\n",
    "        doc.metadata[\"classes\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76400840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'src/stages/data_transformation.py',\n",
       " 'sha': 'b08731404b1cb326434356064b02c9c30ee4dd13',\n",
       " 'source': 'https://api.github.com/rishabhpancholi/fake-job-detection-mlops-project/blob/main/src/stages/data_transformation.py',\n",
       " 'file_type': 'py',\n",
       " 'file_name': 'data_transformation',\n",
       " 'functions': ['__init__',\n",
       "  'lemmatize_text',\n",
       "  'clean_data',\n",
       "  'impute_data',\n",
       "  'transform'],\n",
       " 'classes': ['DataTransformation']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[32].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "900e8ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language = Language.PYTHON,\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "\n",
    "markdown_text_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language = Language.MARKDOWN,\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c6f986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_chunks = python_text_splitter.split_documents([doc for doc in documents if doc.metadata[\"file_type\"] == \"py\"])\n",
    "markdown_chunks = markdown_text_splitter.split_documents([doc for doc in documents if doc.metadata[\"file_type\"]!=\"py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5bdeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# General Imports\\nfrom fastapi import FastAPI\\nfrom fastapi.middleware.cors import CORSMiddleware\\n\\n# Package Imports\\nfrom api.routes import home_router,prediction_router\\nfrom api.exception import register_exception_handlers\\n\\n# Initialize FastAPI\\napp = FastAPI(\\n    title=\"Real/Fake Job Detection API\",\\n    description=\"API for detecting real or fake job postings using NLP and ML models\",\\n)\\n\\n# Adding Middlewares\\napp.add_middleware(\\n    CORSMiddleware,\\n    allow_origins=[\"*\"],\\n    allow_credentials=True,\\n    allow_methods=[\"*\"],\\n    allow_headers=[\"*\"],\\n)\\n\\n# Register Exception Handlers\\nregister_exception_handlers(app)\\n\\n# Include Routers\\napp.include_router(home_router)\\napp.include_router(prediction_router)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_chunks[4].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6497bbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Step 8:\\n      - name: Deploy to EC2 instance\\n        uses: appleboy/ssh-action@v1.0.3\\n        env:\\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\\n          AWS_REGION: ap-south-1\\n        with:\\n          host: ${{ secrets.EC2_HOST }}\\n          username: ubuntu\\n          key: ${{ secrets.EC2_SSH_KEY }}\\n          script: |\\n            echo \"Logging to ECR\"\\n            aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/j4r9x7x2\\n            echo \"Stopping old container\"\\n            docker-compose down\\n            echo \"Removing old image\"\\n            docker image prune -af\\n            echo \"Running new container\"\\n            docker-compose up -d'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_chunks[4].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ac0bca9-7e09-4fce-b12c-3f170666c589",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:6333/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "113e996b-23a7-4770-a571-3a49d578d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4871b7d8-9569-4243-b45c-cdf4ccb3a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_chunks = python_chunks\n",
    "total_chunks.extend(markdown_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75b6c1df-8242-4541-b323-5a853b5cb361",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = QdrantVectorStore.from_documents(\n",
    "    total_chunks,\n",
    "    embeddings,\n",
    "    collection_name=\"codebase_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80c449e1-f39d-432f-a367-8c7aa19a199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs = {\"k\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d4e2e6c-441b-478a-a177-1d03d7da62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which methods are implemented in my model trainer class?\"\n",
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f030e601-5429-4879-a027-56b7af8c7312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'path': 'src/stages/model_trainer.py', 'sha': '57ce1bd7e47f68ad94d96d9469657aa2d8d5280d', 'source': 'https://api.github.com/rishabhpancholi/fake-job-detection-mlops-project/blob/main/src/stages/model_trainer.py', 'file_type': 'py', 'file_name': 'model_trainer', 'functions': ['__init__', 'build_model_pipeline', 'train_model', 'log_experiment', 'train'], 'classes': ['ModelTrainer'], '_id': '1512e0d8-4c8d-49ae-be5e-e6d0f6d0a376', '_collection_name': 'codebase_embeddings'}, page_content='class ModelTrainer:\\n    \"\"\"Class for model trainer stage\"\"\"\\n    def __init__(self, logger = get_logger(\\'model_trainer\\'), utils = Utility()):\\n        \"\"\"Initializes the model trainer class\"\"\"\\n        try:\\n            self.transformed_train_file_path = Path(ARTIFACTS_DIR)/DATA_TRANSFORMATION_DIR_NAME/DATA_TRANSFORMATION_TRANSFORMED_TRAIN_FILE_NAME\\n            self.transformed_test_file_path = Path(ARTIFACTS_DIR)/DATA_TRANSFORMATION_DIR_NAME/DATA_TRANSFORMATION_TRANSFORMED_TEST_FILE_NAME\\n            self.model_file_path = Path(ARTIFACTS_DIR)/MODEL_TRAINER_DIR_NAME/MODEL_TRAINER_MODEL_FILE_NAME\\n            self.target_col = MODEL_TRAINER_TARGET_COL\\n            self.count_vectorizer_max_features = MODEL_TRAINER_COUNT_VECTORIZER_MAX_FEATURES\\n            self.model_name = MODEL_TRAINER_MODEL_NAME\\n            self.registered_model_name = MODEL_TRAINER_REGISTERED_MODEL_NAME\\n            self.model_params = MODEL_TRAINER_MODEL_PARAMS'),\n",
       " Document(metadata={'path': 'src/stages/model_trainer.py', 'sha': '57ce1bd7e47f68ad94d96d9469657aa2d8d5280d', 'source': 'https://api.github.com/rishabhpancholi/fake-job-detection-mlops-project/blob/main/src/stages/model_trainer.py', 'file_type': 'py', 'file_name': 'model_trainer', 'functions': ['__init__', 'build_model_pipeline', 'train_model', 'log_experiment', 'train'], 'classes': ['ModelTrainer'], '_id': '90122549-1d89-4c7a-b97a-0e1c12f4dc86', '_collection_name': 'codebase_embeddings'}, page_content='self.model_name = MODEL_TRAINER_MODEL_NAME\\n            self.registered_model_name = MODEL_TRAINER_REGISTERED_MODEL_NAME\\n            self.model_params = MODEL_TRAINER_MODEL_PARAMS\\n            self.mlflow_experiment_name = MODEL_TRAINER_EXPERIMENT_NAME\\n            self.dagshub_repo_owner = MODEL_TRAINER_REPO_OWNER\\n            self.dagshub_repo_name = MODEL_TRAINER_REPO_NAME\\n            self.utils = utils\\n            self.log = logger\\n            self.uri = f\"https://dagshub.com/{self.dagshub_repo_owner}/{self.dagshub_repo_name}.mlflow\"\\n            mlflow.set_tracking_uri(self.uri)\\n            mlflow.set_registry_uri(self.uri)\\n            mlflow.set_experiment(self.mlflow_experiment_name)\\n            self.log.info(\"Model Trainer started\")\\n        except Exception as e:\\n            raise CustomException(e,sys)\\n        \\n    def build_model_pipeline(self)-> Pipeline:\\n        \"\"\"Builds the model pipeline\"\"\"\\n        try:\\n            self.log.info(\"Building the model pipeline\")'),\n",
       " Document(metadata={'path': 'src/stages/model_trainer.py', 'sha': '57ce1bd7e47f68ad94d96d9469657aa2d8d5280d', 'source': 'https://api.github.com/rishabhpancholi/fake-job-detection-mlops-project/blob/main/src/stages/model_trainer.py', 'file_type': 'py', 'file_name': 'model_trainer', 'functions': ['__init__', 'build_model_pipeline', 'train_model', 'log_experiment', 'train'], 'classes': ['ModelTrainer'], '_id': 'e57a92a8-e955-4897-92b9-e0e8537c11d8', '_collection_name': 'codebase_embeddings'}, page_content='model_pipeline = Pipeline(\\n                steps = [\\n                    (\"column_transformer\",column_transformer),\\n                    (\"model\",model)\\n                ]\\n            )\\n            self.log.info(\"Pipeline built successfully\")\\n            return model_pipeline\\n        except Exception as e:\\n            raise CustomException(e,sys)\\n        \\n    def train_model(self, X_train: pd.DataFrame, y_train: pd.Series, model_pipeline: Pipeline):\\n        \"\"\"Trains the model\"\"\"\\n        try:\\n            self.log.info(\"Training the model\")\\n            model_pipeline.fit(X_train,y_train)\\n            self.log.info(\"Model trained successfully\")\\n        except Exception as e:\\n            raise CustomException(e,sys)\\n        \\n    def log_experiment(self, model_pipeline: Pipeline, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series):\\n        \"\"\"Logs the parameters and the metrics of the model\"\"\"\\n        try:'),\n",
       " Document(metadata={'path': 'src/stages/model_trainer.py', 'sha': '57ce1bd7e47f68ad94d96d9469657aa2d8d5280d', 'source': 'https://api.github.com/rishabhpancholi/fake-job-detection-mlops-project/blob/main/src/stages/model_trainer.py', 'file_type': 'py', 'file_name': 'model_trainer', 'functions': ['__init__', 'build_model_pipeline', 'train_model', 'log_experiment', 'train'], 'classes': ['ModelTrainer'], '_id': 'e268c88d-5872-46fa-aaa8-1c5096d65766', '_collection_name': 'codebase_embeddings'}, page_content='self.utils.save_joblib_file(model_pipeline,self.model_file_path)\\n        except Exception as e:\\n            raise CustomException(e,sys)\\n        \\nif __name__ == \"__main__\":\\n    model_trainer = ModelTrainer()\\n    model_trainer.train()'),\n",
       " Document(metadata={'path': 'src/constants/__init__.py', 'sha': '23172e23297127967d0a2c1769d2f3f40e2b975b', 'source': 'https://api.github.com/rishabhpancholi/fake-job-detection-mlops-project/blob/main/src/constants/__init__.py', 'file_type': 'py', 'file_name': '__init__', 'functions': [], 'classes': [], '_id': 'b0f2113d-01e5-4c1e-a56a-d0c3854f4593', '_collection_name': 'codebase_embeddings'}, page_content='\"\"\"Data Transformation related constants\"\"\"\\nDATA_TRANSFORMATION_DIR_NAME: str = \"data_transformation\"\\nDATA_TRANSFORMATION_TRANSFORMED_TRAIN_FILE_NAME: str = \"transformed_train.csv\"\\nDATA_TRANSFORMATION_TRANSFORMED_TEST_FILE_NAME: str = \"transformed_test.csv\"\\n\\n\"\"\"Model Trainer related constants\"\"\"\\nMODEL_TRAINER_DIR_NAME: str = \"model_trainer\"\\nMODEL_TRAINER_MODEL_FILE_NAME: str = \"model.joblib\"\\nMODEL_TRAINER_COUNT_VECTORIZER_MAX_FEATURES: int = params[\"model_trainer\"][\"COUNT_VECTORIZER_MAX_FEATURES\"]\\nMODEL_TRAINER_MODEL_NAME: str = params[\"model_trainer\"][\"MODEL_NAME\"]\\nMODEL_TRAINER_MODEL_PARAMS: dict = params[\"model_trainer\"][MODEL_TRAINER_MODEL_NAME]\\nMODEL_TRAINER_REGISTERED_MODEL_NAME: str = \"fake-job-detection-model\"\\nMODEL_TRAINER_EXPERIMENT_NAME: str = \"fake-job-detection-experiment\"\\nMODEL_TRAINER_REPO_OWNER: str = os.getenv(\"MODEL_TRAINER_REPO_OWNER\")\\nMODEL_TRAINER_REPO_NAME: str = os.getenv(\"MODEL_TRAINER_REPO_NAME\")\\nMODEL_TRAINER_TARGET_COL: str = \"fraudulent\"')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0d947cdf-7056-44e0-852f-07b24a3aa815",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f449e92-fec1-4c4d-9b50-7916b0624c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualCompressionRetriever:\n",
    "    def __init__(self,base_retriever,compressor,k):\n",
    "        self.base_retriever = base_retriever\n",
    "        self.compressor = compressor\n",
    "        self.k = k\n",
    "\n",
    "    def invoke(self,query):\n",
    "        similar_docs = self.base_retriever.invoke(query)\n",
    "        context = \"\"\n",
    "        for doc in similar_docs:\n",
    "            context += f\"\"\"\n",
    "             Document page content: {doc.page_content}\n",
    "             Document metadata: {doc.metadata}\n",
    "            \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are a specialised contextual compression retriever.\n",
    "        Here is the user's query: {query}\n",
    "        For this query this is the context of every retrieved document along with its metadata:\n",
    "        {context}\n",
    "\n",
    "        I want you to select only {self.k} of them (apply it very strictly) which are most relevant to the users query and return and combine them into one string and return it.\n",
    "\n",
    "        Also mention the source of the files from the metadata(do it strictly).\n",
    "        \"\"\"\n",
    "\n",
    "        result = self.compressor.invoke(prompt)\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cf51e939-7a7a-4828-82d1-8b5af4660fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(retriever,llm,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "07c280d6-0c03-4206-a41b-0332c07278d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a35ed-822a-4deb-930d-86ce9645e3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The continuous deployment process in GitHub Actions is a streamlined, automated workflow designed to ensure that the latest code changes are continuously integrated and deployed. Here's a breakdown of how it works:\\n\\n1.  **Triggering the Workflow**: The entire process kicks off automatically whenever new code is pushed or merged into the `main` branch. This ensures that every significant update to the main development line initiates a deployment cycle.\\n\\n2.  **Artifact Preparation**: Once triggered, the system first logs into Amazon ECR (Elastic Container Registry). It then proceeds to build a Docker image based on the project's codebase. This newly built image, which encapsulates the application and its dependencies, is subsequently pushed to the ECR repository. It's specifically tagged as `public.ecr.aws/j4r9x7x2/rishabhpancholi/real-fake-job-detection-api:latest`, making it readily available for deployment.\\n\\n3.  **Deployment to EC2**: With the Docker image prepared and stored in ECR, the next step is to deploy it to an EC2 instance. This involves several automated actions on the EC2 instance itself:\\n    *   The EC2 instance first logs into ECR to gain access to the Docker image repository.\\n    *   Any previously running Docker containers associated with the application are stopped to ensure a clean deployment.\\n    *   Old and unused Docker images are removed from the EC2 instance to free up resources and prevent conflicts.\\n    *   Finally, the new application container is launched. This command pulls the `latest` Docker image from ECR and starts the application in a detached mode, ensuring the new version is up and running efficiently.\\n\\nThis entire sequence automates the journey of your code from a `push` event to a live application, ensuring rapid and reliable deployments.\\n\\nSource: `https://api.github.com/rishabhpancholi/fake-job-detection-mlops-project/blob/main/.github/workflows/cicd.yml`\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How is the continous deployment working in github actions?\"\n",
    "\n",
    "context = compression_retriever.invoke(query).content\n",
    "\n",
    "prompt = f\"\"\" You are a codebase agent who answers queries regarding certain piece of code. For Eg, How it works and if someone asks about a particular class or function in a file you skillfully handle it and tell it accordingly like an\n",
    "              Excellent teacher.\n",
    "           Here is the entire code context for your reference:\n",
    "           {context}\n",
    "           Now based on this code context this is the user's query:\n",
    "           {query}\n",
    "           Please generate a very good answer for the user.\n",
    "\n",
    "           Please just tell your answer along with the code snippets without telling which document you are referring to (Apply this rule very strictly).\n",
    "           After each answer tell the source provided in the context (again very strict rule).\n",
    "           \"\"\"\n",
    "\n",
    "response = llm.invoke(prompt).content\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
